---
title: "tss_model"
author: "John Gardner"
date: "1/12/2022"
output: html_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(feather)
library(viridis)
library(sf)
library(rgdal)
library(maps)
library(magrittr)
library(mlbench)
library(caret)
library(doParallel)
library(xgboost)
library(Metrics)
library(purrr)
library(data.table)
library(mltools)
library(ggthemes)
library(dplyr)
library(mltools)
library(CAST)
library(future)
library(maptools)

#library(knitr)
#library(randomForest)
#library(onehot)
#library(rnaturalearth)
#library(rnaturalearthdata)
# library(dataRetrieval)
#library(tmap)
knitr::opts_chunk$set(echo = TRUE)
```


```{r prepdata}

iter<- "v1.1"
matchups <- read_csv('out/amazon_matchups_v1.csv')

ggplot(matchups)+
  geom_point(aes(x=red, y=tss)) +
  scale_y_log10()

# split data representatively across time, space, concentration for train/validation
holdout <- function(x) {

  x <- x %>%
  group_by(long_group, time_group) %>%
  dplyr::mutate(mag = cut(tss, quantile(
  x = tss,
  c(0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1),
  include.lowest = T
  )),
  mag = factor(
  mag,
  labels = c(0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1)
  )) %>%
  ungroup()
  
  set.seed(22)
  
  train <- x %>%
  group_by(time_group, long_group, mag) %>%
  sample_frac(.9) %>%
  ungroup() %>%
  dplyr::mutate(.partitions = 1)
  
  validate <- x %>%
   anti_join(train) %>%
   dplyr::mutate(.partitions = 2)

  out <- train %>%
  bind_rows(validate) 
    
  return(out)
}

# make training and validation data
df <- matchups %>%
  filter(!is.na(tss)) %>%
  filter(tss < 7500 & tss >=0.01) %>%
  filter(tss != 1)  %>%
  filter(!rn %in% outlier_sum$uniqueID) %>%
  mutate(lat_group = cut_number(latitude, 2, right= F),
         long_group = cut_number(longitude, 2, right=F),
         date = as.Date(dt_utc),
         julian = as.numeric(julian.Date(date)),
         space_group = paste0(lat_group,long_group),
         time_group = cut_number(julian, 3, right=F)) %>%
         holdout() %>% 
         mutate(tss = log(tss)) %>%
         as.data.frame()

train <- df %>%
  filter(.partitions ==1) %>% 
  ungroup() %>%
  as.data.frame()

validate <- df %>%
  filter(.partitions ==2) %>%
  ungroup() %>%
  as.data.frame()

val.cols <- df %>% 
  filter(.partitions ==2) %>%
  ungroup() 

```

```{r}
# make map of matchup locations
data_map <- df %>% 
  group_by(siteID,latitude, longitude) %>%
  summarise(Count = n()) %>%
  st_as_sf(coords = c(x="longitude", y="latitude"), crs=4326) %>%
  st_transform(4326) 
 # mutate(Count = log10(Count))


sa <- ne_countries(continent = "South America", returnclass = "sf") %>%
    st_transform(4326)

ggplot() +
  geom_sf(data=sa) +
  geom_sf(data=data_map, aes(color=Count), alpha=0.5, size=1) +
  scale_color_viridis_c() + #(breaks = c(0,1,100), labels=c(1, 10, 100)) +
  theme(legend.position = c(0.98, 0.29),
    line = element_blank(), 
    rect = element_blank(), 
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.background = element_blank(),
    legend.text = element_text(size=9))


#ggsave("D:/Dropbox/projects/riverTSS/figs/Amazon_sampling_density.png", width=4, height=6, units="in", dpi=300)


```



```{r ffs, echo=F, message=F}

# select features to use in feature selection
features <- train %>%
  dplyr::select( blue, green, red, nir, swir1, swir2, NR:dw) %>%
  dplyr::select(-NS_NR) %>%
  names(.)

#
set.seed(10)

folds <- CreateSpacetimeFolds(train,
  spacevar = "long_group",
  timevar = "time_group" ,
  k = 2)
  
control <- trainControl(
  method = "cv",
  savePredictions = 'none',
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  p = 0.8)
  
  ## Do initial feature selection with conservative hyperparameters
tuneGrid1 <- expand.grid(
  nrounds = 50,
  eta = .3,
  lambda = 1,
  alpha = 0)


# Set it up to run in parallel. This can take 1-2 days.
cl <- makePSOCKcluster(availableCores() - 2)
registerDoParallel(cl)

ffs <- ffs(train[,features], train$tss, method = 'xgbLinear', metric = 'RMSE', tuneGrid = tuneGrid1, Control = control, verbose = T)

on.exit(stopCluster(cl))
registerDoSEQ()

ffsResults <- ffs$perf_all

# Save the results
#write_csv(ffsResults, "out/ffsResults_v1.1.csv")

ffsResults %>%
  group_by(nvar) %>%
  summarise(RMSE = median(RMSE),
            SE = median(SE)) %>%
  ggplot(.) + geom_line(aes(x = nvar, y = RMSE)) +
  geom_errorbar(aes(x = nvar, ymin = RMSE - SE, ymax = RMSE + SE), color = 'red')

#ggsave(paste0('figs/rfeRMSE_', iter, '.png'), device = 'png', width = 6, height = 4, units = 'in')

```


```{r final_model}

features_1 <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  dplyr::select(-c(nvar, RMSE, SE)) %>%
  mutate(cna = rowSums(is.na(.)) ) %>%
  filter(cna == max(cna)) %>%
  select(-cna) %>%
  paste(.) %>% .[.!= 'NA'] %>%
  as.vector()

grid_base <- expand.grid(
  nrounds = c(50,100,200,300),
  alpha = c(0, 0.001, 0.01, 1),
  lambda = c(0.01, 0.1, 1, 5, 10),
  eta = c(0.01, 0.1, 0.3, 0.6 )
)

set.seed(10)

folds <- CreateSpacetimeFolds(train, spacevar = "long_group", timevar = "time_group" , k=6)

##Set up a cluster to run everything in parallel
#cl <- makePSOCKcluster(availableCores()-2)
#registerDoParallel(cl)

train_control <- caret::trainControl(
  method = "cv",
  savePredictions = F,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  verboseIter = T,
  allowParallel = TRUE,
  p = 0.8
  )
  
base_model <- caret::train(
  objective ='reg:squarederror',
  x = train[,features_1],   
  y = train$tss,
  trControl = train_control,
  tuneGrid = grid_base,
  method = "xgbLinear",
  verbose = TRUE,
 # preProcess = c('center', 'scale'),
  importance = F
)

base_model$bestTune

# 
train_control_final <- caret::trainControl(
 
  method = "cv",
  savePredictions = T,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  verboseIter = T,
  allowParallel = TRUE,
  p = 0.8
  )
  
grid_final <- expand.grid(
  nrounds = base_model$bestTune$nrounds,
  alpha = base_model$bestTune$alpha,
  lambda = base_model$bestTune$lambda,
  eta = base_model$bestTune$eta
)
  
model <- caret::train(
  objective ='reg:squarederror',
  x = train[,features_1],
  y = train$tss,
  trControl = train_control_final,
  tuneGrid = grid_final,
  method = "xgbLinear",
 # preProcess = c('center', 'scale'),
  importance = T,
  verbose = TRUE
)

#stopCluster(cl)

# helper function for plot
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(base_model)

#ggsave(paste0("out",iter, ".tiff"), width=10, height = 8, units="in", dpi=300)


```


```{r save}

pred<- predict( model, validate[,features_1])
actual <- (validate$tss)
uniqueID <- val.cols$rn

output <- tibble(Predicted = pred, Actual = actual, uniqueID = uniqueID) %>%
  mutate(Actual = exp(Actual), Predicted = exp(Predicted)) %>%
  left_join(matchups, by=c("uniqueID"="rn")) %>%
  mutate(residual = Actual - Predicted,
         year = lubridate::year(dt_utc),
         month = month(dt_utc),
         obs = ifelse(abs(residual) > quantile(abs(residual), .975, na.rm=T), "bad", "good")) 

### save all training data
#write_feather(train, path ='D:/Dropbox/projects/tss_amazon/out/train_20220101.feather'))
#write_csv(train, 'out/train_full_v1.1.csv')

### save just features and response variable that are used
#write_feather(train %>% select(value, features_1), path = paste0('D:/Dropbox/projects/riverTSS/4_model/out/train_clean_',iter, '.feather'))
#write_csv(train %>% select(tss, features_1), path ='out/train_v1.1.csv')

### save output of validation model
#write_csv(output, path ='out/validation_v1.1.csv')

### save the model
#save(model, file='D:/Dropbox/projects/tss_amazon/out/tssAmazon_model_v1.1.rda')
#saveRDS(model, file='D:/Dropbox/projects/tss_amazon/out/tssAmazon_model_v1.1.RDS')

```


```{r eval}
output <- read_csv('out/validation_v1.1.csv')

Rerror <- function(y, y_hat) {
   # called "% Bias" in from Dethier et al. 2020
   x<- (10^median(abs(log10(10^y_hat/10^y)), na.rm = T)-1)

return(x)
}

evals <- output %>%
  mutate(Actual = (Actual), 
         Predicted = (Predicted)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted),
            Rerr = Rerror(log10(Actual), log10(Predicted))) 

evals 

mod <-lm(log10(Predicted) ~ log10(Actual), data = output)
summary(mod)

ggplot(output,

       aes(x = (Actual), y = (Predicted))) + 
  scale_x_log10()+
  scale_y_log10() +
  geom_point() +
  scale_fill_viridis(name = 'Log10(Count)') + 
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured SSC (mg/L)") +
  ylab("Predicted SSC (mg/L)")+
  theme_few() +
  theme(text = element_text(size=18),
        legend.position = c(0.25,0.75),
        legend.background = element_blank()) 
  #facet_wrap(~sat)

#ggsave('D:/Dropbox/projects/tss_amazon/figs/ModelValidation_amazon_v1.1.png', width = 5, height = 5, units = 'in' ,dpi=350)

ggplot(output, aes(x = year, y = residual)) + 
  geom_point() +
  xlab("year") +
  ylab("Residual")+
  theme_few() +
  theme(text = element_text(size=18))
  
errorSum <- output %>%
  filter( Actual < 5000) %>%
  mutate(Observed.Value = Actual) %>%
  rename(Year = year, Latitude = latitude, Longitude = longitude) %>%
  mutate(Longitude = round(Longitude, 1)) %>%
  gather(Observed.Value, Year, Latitude, Longitude, key = 'Variable', value = 'Value') %>%
  group_by(Variable) %>%
  mutate(quantile = ifelse(Variable == "Observed.Value", 
                   cut_number(Value, n=5, right = F, breaks= c(0, 10, 100,1000, 10000), labels = F),
                   cut_number(Value, n=4, right = F, labels = F))) %>%
    mutate(quantLabs = ifelse(Variable == "Observed.Value", as.character(case_when(
      quantile == 1 ~ "0-10",
      quantile == 2 ~ "10-100",
      quantile == 3 ~ "100-1000",
      quantile == 4  ~ "1k-10k"
    )),
    as.character(cut_number(Value, 4,  right = F, dig.lab = 3)))) %>% 
 ungroup() %>%
  group_by(quantile, quantLabs, Variable) %>%
  dplyr::summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            bias = bias(Actual, Predicted),
            smape = smape(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted)
            ) %>%
  gather(rmse:p.bias, key = 'Metric', value = 'Error') %>%
  as.data.frame() %>%
  arrange(Variable, quantile) %>%
  mutate(order = row_number())

ggplot(errorSum %>% 
         filter(quantLabs != "outlier") %>%
         filter(Metric == 'smape'|Metric =='p.bias'|Metric=="mae") %>%
         mutate(Error = ifelse(Metric %in% c("smape", "p.bias"), Error*100, Error)), aes(x = fct_reorder(quantLabs,order), y = Error, color = Metric, group = Metric)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~Variable, scales = 'free')  +  
  theme_bw() +
  scale_color_viridis(discrete = T) +
  #scale_y_continuous(labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5),
        legend.position = 'bottom') +
  labs(x = 'Quantile', y = 'Error (% or mg/L)')

#ggsave("D:/Dropbox/projects/tss_amazon/figs/error_groups_v1.1.tiff", width=6, height = 6, dpi = 300, units= "in")


model$modelInfo$varImp(model$finalModel) %>%
  mutate(Feature = fct_reorder(rownames(.), Overall, .desc = F)) %>%
  arrange(Overall) %>%
  #left_join(tibble(feature_1 = model$modelInfo$predictors(model$finalModel), Feature = c('Nir', "Red/(Blue+Swir1)", 'Swir2', "Nir-Red", 'Blue/Green', 'Swir1', 'Red/Green', "Brightness")), by="feature") %>%
  ggplot(., aes(x = Feature, y = Overall)) + 
  geom_col() +
#  scale_x_discrete(labels = rev(c('Hue', 'Nir/(Blue+Swir1)',"Red/(Blue+Swir1)", "Brightness", "Nir-Swir1" ,"Green/(Blue+Red)",'Blue/Green',"Nir/Red")) ) +  #For final make labels nice
  coord_flip() +
  theme_bw() +
  labs(y = 'Importance (Model Gain)') 

#ggsave("D:/Dropbox/projects/tss_amazon/figs/feat_importance_v1.1.png", width=4, height = 3, dpi = 300, units= "in")


```


```{r predict, echo=F, message=F}

# load data SR data
sr_clean_all <- read_feather("D:/Dropbox/projects/tss_amazon/out/sr_amazon_v1.feather") 

model <- readRDS('D:/Dropbox/projects/tss_amazon/out/tssAmazon_model.RDS')

features_1 <- model$finalModel$feature_names
  
sr_pred <- tibble(tss = exp(predict(model, sr_clean_all[,features_1])))

hist(log10(sr_pred$tss), breaks = 20)

sr_tss <- sr_clean_all %>%
  bind_cols(sr_pred) %>%
  distinct(reach_ID, date, path, row, sat, blue, .keep_all = TRUE) %>%
  dplyr::select(LS_ID, reach_ID, date, year, month, tss, CLOUD_COVER, SOLAR_AZIMUTH_ANGLE, SOLAR_ZENITH_ANGLE, hillshadow, dswe, path, row, sat, pCount_dswe1, count, pixel_qa, rn)

#write_csv(sr_tss, "D:/Dropbox/projects/tss_amazon/out/RiverSed_Amazon_V1.1.txt")

#write_feather(sr_tss, "D:/Dropbox/projects/tss_amazon/out/RiverSed_Amazon_V1.1.feather")


```





